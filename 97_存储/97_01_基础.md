
Prometheus提供了本地存储，即tsdb时序数据库，本地存储给Prometheus带来了简单高效的使用体验，prometheus2.0以后压缩数据能力也得到了很大的提升。可以在单节点的情况下满足大部分用户的监控需求。

但本地存储也限制了Prometheus的可扩展性，带来了数据持久化等一系列的问题。为了解决单节点存储的限制，prometheus没有自己实现集群存储，而是提供了远程读写的接口，让用户自己选择合适的时序数据库来实现prometheus的扩展性。

Prometheus 1.x版本的TSDB（V2存储引擎）基于LevelDB，并且使用了和Facebook Gorilla一样的压缩算法，能够将16个字节的数据点压缩到平均1.37个字节。

Prometheus 2.x版本引入了全新的V3存储引擎，提供了更高的写入和查询性能，经过使用发现查询更快，用户体验大升级。

容器版本的promtheus这次也同步升级到了2.23，解决以前单个service pod出现内存OOM的问题，现在内存使用比较稳定，不会出现以前内存占用步步高升的情况了。

另外7.x版本的Grafana UI效果和配色总体感觉比6.x看起来偏冷，扁平化明显。


```
# 启动参数
./prometheus 
--config.file=./prometheus.yml 
--web.listen-address=0.0.0.0:19091 
--web.enable-lifecycle 
--storage.tsdb.path=/data/PromDB 
--storage.tsdb.retention.time=30d 
--query.timeout=2m 
--log.level=info 
--log.format=ogfmt 
--storage.tsdb.retention.size=2TB 
--storage.tsdb.no-lockfile 
--storage.tsdb.wal-compression --rules.alert.resend-delay=5s
```

# 1 概述

Prometheus之于kubernetes(监控领域)，如kubernetes之于容器编排。  
随着heapster不再开发和维护以及influxdb 集群方案不再开源，heapster+influxdb的监控方案，只适合一些规模比较小的k8s集群。而prometheus整个社区非常活跃,除了官方社区提供了一系列高质量的exporter，例如node_exporter等。Telegraf(集中采集metrics) + prometheus的方案，也是一种减少部署和管理各种exporter工作量的很好的方案。  
今天主要讲讲我司在使用prometheus过程中，存储方面的一些实战经验。



# 2 Prometheus 储存瓶颈


![](image/Pasted%20image%2020240704212037.png)

通过prometheus的架构图可以看出，prometheus提供了本地存储，即tsdb时序数据库。本地存储的优势就是运维简单,缺点就是无法海量的metrics持久化和数据存在丢失的风险，我们在实际使用过程中，出现过几次wal文件损坏，无法再写入的问题。  
当然prometheus2.0以后压缩数据能力得到了很大的提升。为了解决单节点存储的限制，prometheus没有自己实现集群存储，而是提供了远程读写的接口，让用户自己选择合适的时序数据库来实现prometheus的扩展性。

prometheus通过下面两种方式来实现与其他的远端存储系统对接

- Prometheus 按照标准的格式将metrics写到远端存储
- prometheus 按照标准格式从远端的url来读取metrics


![](image/Pasted%20image%2020240704212116.png)



# 3 metrics的持久化的意义和价值

 其实监控不仅仅是体现在可以实时掌握系统运行情况，及时报警这些。而且监控所采集的数据，在以下几个方面是有价值的
- 资源的审计和计费。这个需要保存一年甚至多年的数据的。
- 故障责任的追查
- 后续的分析和挖掘，甚至是利用AI，可以实现报警规则的设定的智能化，故障的根因分析以及预测某个应用的qps的趋势，提前HPA等，当然这是现在流行的AIOPS范畴了。


# 4 Prometheus 数据持久化方案

## 4.1 方案选型

社区中支持prometheus远程读写的方案

- AppOptics: write
- Chronix: write
- Cortex: read and write
- CrateDB: read and write
- Elasticsearch: write
- Gnocchi: write
- Graphite: write
- InfluxDB: read and write
- OpenTSDB: write
- PostgreSQL/TimescaleDB: read and write
- SignalFx: write
- clickhouse: read and write

## 4.2 选型方案需要具备以下几点

- 满足数据的安全性，需要支持容错，备份
- 写入性能要好，支持分片
- 技术方案不复杂
- 用于后期分析的时候，查询语法友好
- grafana读取支持，优先考虑
- 需要同时支持读写

基于以上的几点，[clickhouse](https://clickhouse.yandex/)满足我们使用场景。  
Clickhouse是一个高性能的列式数据库，因为侧重于分析，所以支持丰富的分析函数。

下面是Clickhouse官方推荐的几种使用场景：

- Web and App analytics
- Advertising networks and RTB
- Telecommunications
- E-commerce and finance
- Information security
- Monitoring and telemetry
- **Time series**
- Business intelligence
- Online games
- Internet of Things

ck适合用于存储Time series  
此外社区已经有graphouse项目，把ck作为Graphite的存储。


# 5 性能测试

https://www.cnblogs.com/JetpropelledSnake/p/10432557.html


# 6 方案设计

![](image/Pasted%20image%2020240704212409.png)

关于此架构，有以下几点：
- 每个k8s集群部署一个Prometheus-clickhouse-adapter 。关于Prometheus-clickhouse-adapter该组件，下面我们会详细解读。
- clickhouse 集群部署，需要zk集群做一致性表数据复制。


而clickhouse 的集群示意图如下：
![](image/Pasted%20image%2020240704212614.png)

- ReplicatedMergeTree + Distributed。ReplicatedMergeTree里，共享同一个ZK路径的表，会相互，注意是，相互同步数据
- 每个IDC有3个分片，各自占1/3数据
- 每个节点，依赖ZK，各自有2个副本



 **zk集群部署注意事项**：
- 安装 ZooKeeper 3.4.9或更高版本的稳定版本
- 不要使用zk的默认配置，默认配置就是一个定时炸弹。





# 7 Prometheus-Clickhuse-Adapter组件


Prometheus-Clickhuse-Adapter(Prom2click) 是一个将clickhouse作为prometheus 数据远程存储的适配器。  
[prometheus-clickhuse-adapter](https://github.com/iyacontrol/prom2click)，该项目缺乏日志，对于一个实际生产的项目，是不够的，此外一些数据库连接细节实现的也不够完善，已经在实际使用过程中将改进部分作为pr提交。

在实际使用过程中，要注意并发写入数据的数量，及时调整启动参数ch.batch 的大小，实际就是批量写入ck的数量，目前我们设置的是65536。因为ck的Merge引擎有一个300的限制，超过会报错
Too many parts (300). Merges are processing significantly slower than inserts

300是指 processing，不是指一次批量插入的条数。






